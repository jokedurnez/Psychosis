{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the psychosis dataset with neuropsych measures\n",
    "\n",
    "This was my first attempt to address the whole database.  It didn't work and we worked out a few subscales.  So this notebook is probably useless at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from sklearn.decomposition import FactorAnalysis, PCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from utils.prog import log_progress as lp\n",
    "import statsmodels.formula.api as smf\n",
    "from numpy import transpose as T\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import statsmodels.api as sm\n",
    "from utils import neuropsych\n",
    "import scipy.stats as stats\n",
    "import matplotlib as mpl\n",
    "import sklearn.cluster\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import palettable\n",
    "import warnings\n",
    "import scipy\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext rpy2.ipython\n",
    "\n",
    "#cols = palettable.colorbrewer.qualitative.Dark2_8.hex_colors\n",
    "cols = palettable.tableau.ColorBlind_10.hex_colors\n",
    "cols += palettable.tableau.PurpleGray_6.hex_colors\n",
    "cols += palettable.tableau.Tableau_10.hex_colors\n",
    "\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "CONDIR = os.environ.get(\"CONDIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table = \"TRAINTABLE\"\n",
    "RC = pd.read_csv(os.environ.get(table),low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " From here on, we only work with the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CONDIR = '/scratch/users/jdurnez/Psychosis_derivatives/connectivity_Joke/'\n",
    "table = 'TRAINTABLE' if True else 'CLEANTABLE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare neuropsych measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions grab and clean the data to immediately work with.  These functions are stored in utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RC, labeltable = neuropsych.get_tables(table)\n",
    "labeltable, RC_table = neuropsych.subset_tables(table=table, \n",
    "                                                RC=RC, \n",
    "                                                labeltable=labeltable)\n",
    "\n",
    "RC_table[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I'm subsetting all psych variables, to include only certain scales, an only numerical values.  Then, I'm standardising the table, so that the weights of any data reduction are comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "Some columns pose problems for factor analysis (matrix singularity):\n",
    "1. There seem to be columns that are empty for all subjects.  These should be removed.\n",
    "2. There is an extremely high collinearity for certain columns (often _severity_ and _duration_ of same variable).  You can see these in the correlation plot below: the little brown squares in the ldps scale.  These should be removed or be summarised.\n",
    "\n",
    "This can be seen in the correlation matrix of the data matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cormat = RC_table.corr()\n",
    "cormat.fillna(0)\n",
    "unique_labels = np.unique(labeltable.scale)\n",
    "labels_num = [np.where(unique_labels==x)[0].tolist()[0] for val,x in enumerate(labeltable.scale)]\n",
    "major_ticks = [np.min(np.where(np.array(labels_num)==x))-1 for x in range(len(np.unique(labeltable.scale)))]\n",
    "minor_ticks = [np.mean(np.where(np.array(labels_num)==x))-1 for x in range(len(np.unique(labeltable.scale)))]\n",
    "fig = plt.figure(figsize=(8, 6), dpi= 100, facecolor='w', edgecolor='k')\n",
    "ax = fig.add_subplot(111)\n",
    "ax1 = ax.imshow(cormat,cmap = \"PuOr_r\",vmin=-0.2,vmax=1,aspect='auto',interpolation='nearest')\n",
    "ax.set_title('correlations between neuropsych measures')\n",
    "ax.set_xticks(major_ticks,minor=False)\n",
    "ax.set_xticks(minor_ticks,minor=True)\n",
    "ax.set_xticklabels(unique_labels,minor=True,rotation=90)\n",
    "ax.set_xticklabels(unique_labels,minor=False,visible=False)\n",
    "ax.set_yticks(major_ticks,minor=False)\n",
    "ax.set_yticks(minor_ticks,minor=True)\n",
    "ax.set_yticklabels(unique_labels,minor=True)\n",
    "ax.set_yticklabels(unique_labels,minor=False,visible=False)\n",
    "plt.colorbar(ax1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following funcion removed the empty columns and removes columns with really high multicollinearity (defined by the variable `cor_limit`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeltable, FA_table = neuropsych.clean_tables(labeltable, \n",
    "                                               RC_table,\n",
    "                                               cor_lim=0.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# psydict = os.path.join(os.environ.get(\"TABLEDIR\"),'psychosis_dict.csv')\n",
    "# psydict = pd.read_csv(psydict)\n",
    "# psydict['scale']=''\n",
    "# for idx,row in psydict.iterrows():\n",
    "#     whichscale = np.where(row.code==labeltable.code)[0]\n",
    "#     psydict.set_value(idx,'scale',labeltable.scale[whichscale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks better..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cormat = FA_table.corr()\n",
    "cormat.fillna(0)\n",
    "unique_labels = np.unique(labeltable.scale)\n",
    "labels_num = [np.where(unique_labels==x)[0].tolist()[0] for val,x in enumerate(labeltable.scale)]\n",
    "major_ticks = [np.min(np.where(np.array(labels_num)==x))-1 for x in range(len(np.unique(labeltable.scale)))]\n",
    "minor_ticks = [np.mean(np.where(np.array(labels_num)==x))-1 for x in range(len(np.unique(labeltable.scale)))]\n",
    "fig = plt.figure(figsize=(8, 6), dpi= 100, facecolor='w', edgecolor='k')\n",
    "ax = fig.add_subplot(111)\n",
    "ax1 = ax.imshow(cormat,cmap = \"PuOr_r\",vmin=-0.2,vmax=1,aspect='auto',interpolation='nearest')\n",
    "ax.set_title('correlations between neuropsych measures')\n",
    "# ax.set_xticks(major_ticks,minor=False)\n",
    "# ax.set_xticks(minor_ticks,minor=True)\n",
    "# ax.set_xticklabels(unique_labels,minor=True,rotation=90)\n",
    "# ax.set_xticklabels(unique_labels,minor=False,visible=False)\n",
    "# ax.set_yticks(major_ticks,minor=False)\n",
    "# ax.set_yticks(minor_ticks,minor=True)\n",
    "# ax.set_yticklabels(unique_labels,minor=True)\n",
    "# ax.set_yticklabels(unique_labels,minor=False,visible=False)\n",
    "plt.colorbar(ax1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First factor analysis pass\n",
    "\n",
    "We'll try a factor analysis on the whole table.  **Note: $p>n$**, the number of features is larger than the number of measurements.  As such, it will be hard to fit a factor analysis on the whole matrix.  Even if it would be possible, the results are very unstable, what can be seen in the example below.  Including more than one factor leads to a high variability of the cross-validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def factor_ncomponents(X,n_components,cvgroups=3):\n",
    "    fa = FactorAnalysis()\n",
    "    fa.svd_method='randomized'\n",
    "    fa.iterated_power=2\n",
    "    fa.random_state = 100\n",
    "\n",
    "    fa_mean_scores = []\n",
    "    fa_std_scores = []\n",
    "    maxlog = []\n",
    "    for n in lp(n_components):\n",
    "        fa.n_components = n\n",
    "        cvscore = cross_val_score(fa, X,cv=cvgroups)\n",
    "        fa_mean_scores.append(np.mean(cvscore))\n",
    "        fa_std_scores.append(np.std(cvscore))\n",
    "        fa.fit(X)\n",
    "        maxlog.append(fa.loglike_[-1])\n",
    "\n",
    "    return fa_mean_scores, fa_std_scores, maxlog\n",
    "\n",
    "def plot_components(fa_mean_scores, fa_std_scores, maxlog):\n",
    "    fa_z = (np.array(fa_mean_scores)-np.mean(fa_mean_scores))/np.std(fa_mean_scores)\n",
    "    fastd_z = np.array(fa_std_scores)/np.std(fa_mean_scores)/len(fa_mean_scores)\n",
    "    maxlog_z = (np.array(maxlog)-np.mean(maxlog))/np.std(maxlog)\n",
    "    n_components_fa = n_components[np.argmax(fa_mean_scores)]\n",
    "    n_components_ML = n_components[np.argmax(maxlog_z)]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(n_components, maxlog_z, cols[0], label='training likelihood')\n",
    "    plt.axvline(n_components_ML, color=cols[0],\n",
    "                label='FactorAnalysis loglikelihood: %d' % n_components_ML,\n",
    "                linestyle='--')\n",
    "    plt.plot(n_components, fa_z, cols[1], label='testing likelihood')\n",
    "    #plt.plot(n_components, fa_z-2*fastd_z, cols[1], label='95 CI',linestyle=':')\n",
    "    #plt.plot(n_components, fa_z+2*fastd_z, cols[1],linestyle=':')\n",
    "    plt.axvline(n_components_fa, color=cols[1],\n",
    "                label='FactorAnalysis CV: %d' % n_components_fa,\n",
    "                linestyle='--')\n",
    "\n",
    "    plt.xlabel('Number of components')\n",
    "    plt.ylabel('Standardized scores')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title(\"Number of components based on cross-validation and maximum likelihood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The measures that are shows are the loglikelihood of the training data (thus the obtained maximum loglikelihood, which can be expected to keep going up with more factors) and the loglikelihood of the testing data given the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#n_components = np.arange(1,15,2)\n",
    "\n",
    "ncomponents = pd.read_csv(os.path.join(os.environ.get(\"MLDIR\"),'ncomponents.csv'))\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.plot(ncomponents.n_components[:30],ncomponents.fa_scores[:30])\n",
    "plt.title(\"Leave-1-out cross validation loglikelihood score for number of components.\")\n",
    "plt.xlabel(\"number of components\")\n",
    "plt.ylabel('CV loglikelihood')\n",
    "#fa_mean_scores, fa_std_scores, maxlog = factor_ncomponents(X,n_components,cvgroups=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure shows that already after one factor, the cross-validation loglikelihood reduced.  Given the high dimensionality of this dataset, this is very increadible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fa = FactorAnalysis()\n",
    "fa.svd_method='randomized'\n",
    "fa.iterated_power=3\n",
    "fa.random_state = 100\n",
    "fa.n_components = 6\n",
    "fa.fit(FA_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_red = fa.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "codes = list(FA_table.columns)\n",
    "labels = [psych_labels[x] for x in codes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this code makes nice figures of factors and their loadings\n",
    "minor_ticks = np.arange(len(labels))\n",
    "\n",
    "labelnew = [x[:50] for x in labels]\n",
    "fig = plt.figure(figsize=(10, 30), dpi= 100, facecolor='w', edgecolor='k')\n",
    "ax = fig.add_subplot(111)\n",
    "ax1 = ax.imshow(np.transpose(X_red),\n",
    "                aspect='auto',interpolation='nearest',cmap='coolwarm')\n",
    "ax.set_yticks(np.arange(X_red.shape[1]),minor=True)\n",
    "ax.set_yticklabels(labelnew,minor=True)\n",
    "plt.colorbar(ax1)\n",
    "\n",
    "\n",
    "# coldict = {k:cols[idx] for idx,k in enumerate(np.unique(labeltable.scale))}\n",
    "# labeltable['colors'] = [coldict[x] for x in labeltable.scale]\n",
    "# for ytick, color in zip(ax.get_yticklabels(minor=True), labeltable.colors):\n",
    "#     ytick.set_color(color)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%R -i FA_table -o factortable\n",
    "#\n",
    "#maxfac <- 5\n",
    "#\n",
    "#library(psych)\n",
    "#factortable <- nfactors(FA_table,n=maxfac,rotation='varimax')$vss.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to fit a data reduction at the scale of the scales :-) .\n",
    "We'll first try to fit a factor analysis to each scale, starting with the legal issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "include = ['legal_issues']\n",
    "labeltable, legal_table = neuropsych.subset_tables(include=include,table=table)\n",
    "labeltable, legal_table = neuropsych.clean_tables(labeltable, legal_table,cor_lim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following chunk of code performs a factor analysis in R (in R not python because R gives a lot more information about the number of factors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R -i legal_table -o factortable\n",
    "\n",
    "maxfac <- min(dim(legal_table)[2],50)\n",
    "\n",
    "#library(psych)\n",
    "factortable <- nfactors(legal_table,n=maxfac,rotation='varimax')$vss.stats\n",
    "factortable['eigen'] <- eigen(cor(legal_table))$values[1:maxfac]\n",
    "\n",
    "parallel <- fa.parallel(legal_table,plot=FALSE)\n",
    "factortable['parallel_data'] <- parallel$fa.values[1:maxfac]\n",
    "factortable['parallel_sim'] <- parallel$fa.sim[1:maxfac]\n",
    "\n",
    "vss <- vss(legal_table,n=maxfac,rotate='varimax',plot=FALSE)\n",
    "factortable['map'] <- vss$map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import factanal\n",
    "reload(factanal)\n",
    "\n",
    "minfact = factanal.evaluate_factors(factortable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "sns.distplot(minfact.values(),bins=range(25))\n",
    "plt.xlim([0,25])\n",
    "plt.title(\"number of factors for factor analysis on legal table\")\n",
    "plt.xlabel('n components')\n",
    "plt.ylabel('density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these scores, it's hard to pinoint the number of factors to be considered.  Let's look at the cross-validation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = legal_table\n",
    "n_components = np.arange(0,10,2)\n",
    "\n",
    "fa_mean_scores, fa_std_scores, maxlog = factor_ncomponents(X,n_components,cvgroups=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_components(fa_mean_scores, fa_std_scores, maxlog)\n",
    "\n",
    "minfact['loglikelihood']=n_components[np.where(max(maxlog)==maxlog)]\n",
    "minfact['cross-validation']=n_components[np.where(max(fa_mean_scores)==fa_mean_scores)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All metrics point in different directions.  The cross-validation metric shows that 1 factor should be enough, as does the minimum average partial correlation method (known to be less conservative).  So let's go for the median: 10 factors.  Probably that's too much, but that should show up later.\n",
    "\n",
    "Objective function:\n",
    "\n",
    "$log(trace ((FF'+U2)^{-1} R) - log(|(FF'+U2)^-1 R|) - n.items$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R -i legal_table -o fa_loadings\n",
    "\n",
    "library(psych)\n",
    "\n",
    "factortable <- fa(legal_table,nfactors=2,rotate='cluster')\n",
    "fa_loadings <- factortable$loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X=np.dot(legal_table,fa_loadings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The factor analysis seems to be summarised in the seriousness of the legal history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minor_ticks = np.arange(len(labeltable.code))\n",
    "\n",
    "labelnew = [x[:50] for x in legal_table.columns]\n",
    "fig = plt.figure(figsize=(5, 6), dpi= 100, facecolor='w', edgecolor='k')\n",
    "ax = fig.add_subplot(111)\n",
    "ax1 = ax.imshow(fa_loadings,\n",
    "                aspect='auto',interpolation='nearest',cmap='coolwarm')\n",
    "ax.set_yticks(np.arange(fa_loadings.shape[0]),minor=True)\n",
    "ax.set_yticklabels(labelnew,minor=True)\n",
    "plt.colorbar(ax1)\n",
    "\n",
    "\n",
    "coldict = {k:cols[0] for idx,k in enumerate(np.unique(labeltable.scale))}\n",
    "labeltable['colors'] = [coldict[x] for x in labeltable.scale]\n",
    "for ytick, color in zip(ax.get_yticklabels(minor=True), labeltable.colors):\n",
    "    ytick.set_color(color)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "include = ['family_history_assessment']\n",
    "labeltable, family_table = neuropsych.subset_tables(include=include,table=table)\n",
    "labeltable, family_table = neuropsych.clean_tables(labeltable, family_table,cor_lim=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R -i legal_table -o factortable\n",
    "\n",
    "maxfac <- min(dim(legal_table)[2],50)\n",
    "\n",
    "library(psych)\n",
    "factortable <- nfactors(legal_table,n=maxfac,rotation='varimax')$vss.stats\n",
    "factortable['eigen'] <- eigen(cor(legal_table))$values[1:maxfac]\n",
    "\n",
    "parallel <- fa.parallel(legal_table,plot=FALSE)\n",
    "factortable['parallel_data'] <- parallel$fa.values[1:maxfac]\n",
    "factortable['parallel_sim'] <- parallel$fa.sim[1:maxfac]\n",
    "\n",
    "vss <- vss(legal_table,n=maxfac,rotate='varimax',plot=FALSE)\n",
    "factortable['map'] <- vss$map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import factanal\n",
    "reload(factanal)\n",
    "\n",
    "minfact = factanal.evaluate_factors(factortable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = FA_table\n",
    "n_components = np.arange(0,5,1)\n",
    "\n",
    "fa_mean_scores, fa_std_scores, maxlog = factor_ncomponents(X,n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_components(fa_mean_scores, fa_std_scores, maxlog)\n",
    "\n",
    "minfact['loglikelihood']=n_components[np.where(max(maxlog)==maxlog)]\n",
    "minfact['cross-validation']=n_components[np.where(max(fa_mean_scores)==fa_mean_scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(minfact.values(),bins=range(0,25))\n",
    "#np.median(minfact.values())\n",
    "plt.xlim([0,25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline prediction accuracy based on neuropsych variables\n",
    "\n",
    "Right now, the SVM is having extremely high classification accuracy.  Not really that much of a surprise, given that some forms were only filled out by psych patients (verify !) and the presence of hallucinations is pretty obvious :) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RC = pd.read_csv(os.environ.get(table),low_memory=False)\n",
    "y = RC.is_this_subject_a_patient\n",
    "\n",
    "\n",
    "#X = np.array(FA_table)\n",
    "X= FA_table\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "cv = ShuffleSplit(n_splits=50, test_size=0.2, random_state=100)\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "def plot_learning_curve(estimator,title,X,y,ylim,cv=None,n_jobs=3,train_sizes = np.linspace(0.1,1.0,10)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.ylim(ylim)\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=4, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "    plt.xlabel(\"Training sample size\")\n",
    "    plt.ylabel(\"classification accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "estimator = GaussianNB()\n",
    "plot_learning_curve(estimator,\"Naive Bayes\",X,y,ylim=[0.5,1.05],cv=cv)\n",
    "\n",
    "estimator = LinearSVC()\n",
    "plot_learning_curve(estimator,\"Support Vector Classification\",X,y,ylim=[0.5,1.05],cv=cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%R\n",
    "#leave this in: only way to install package with R --> will need :)\n",
    "#install.packages(\"nFactors\",repos='http://cran.us.r-project.org')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canonical correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "cca = CCA(n_components = 4)\n",
    "cca.fit(X_new,Y)\n",
    "cca.get_params()\n",
    "#cca.x_scores_.shape\n",
    "cca.x_weights_.shape\n",
    "cca.y_weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
