{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use machine learning to evaluate the psychosis dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from sklearn.decomposition import FactorAnalysis, PCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from misc.prog import log_progress as lp\n",
    "import statsmodels.formula.api as smf\n",
    "from numpy import transpose as T\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "from misc import neuropsych\n",
    "import matplotlib as mpl\n",
    "import sklearn.cluster\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import palettable\n",
    "import warnings\n",
    "import scipy\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext rpy2.ipython\n",
    "\n",
    "#cols = palettable.colorbrewer.qualitative.Dark2_8.hex_colors\n",
    "cols = palettable.tableau.ColorBlind_10.hex_colors\n",
    "cols += palettable.tableau.PurpleGray_6.hex_colors\n",
    "cols += palettable.tableau.Tableau_10.hex_colors\n",
    "\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "CONDIR = os.environ.get(\"CONDIR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in connectomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = pd.read_csv(os.path.join(CONDIR,'derivatives/connectome_results.csv'))\n",
    "results = results.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " From now on, we only work with the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CONDIR = '/scratch/users/jdurnez/Psychosis_derivatives/connectivity_Joke/'\n",
    "RESTABLE = pd.read_csv(os.environ.get(\"TRAINTABLE\"))\n",
    "#RESTABLE = RESTABLE[RESTABLE.is_this_subject_a_patient==888]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare neuropsych measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I'm making a table with the codes, the labels and the scale of all neuropsych variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ...getting redcap instruments...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>label</th>\n",
       "      <th>scale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject_id</td>\n",
       "      <td>Subject</td>\n",
       "      <td>subject_status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>redcap_data_access_group</td>\n",
       "      <td>Data Access Group</td>\n",
       "      <td>subject_status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is_this_subject_a_patient</td>\n",
       "      <td>Is this subject a patient, control or drop?</td>\n",
       "      <td>subject_status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>date_testing</td>\n",
       "      <td>Date of Ratings Testing</td>\n",
       "      <td>subject_status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>date_nc_testing</td>\n",
       "      <td>Date of Neurocognitive Testing</td>\n",
       "      <td>subject_status</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        code                                        label  \\\n",
       "0                 subject_id                                      Subject   \n",
       "1   redcap_data_access_group                            Data Access Group   \n",
       "2  is_this_subject_a_patient  Is this subject a patient, control or drop?   \n",
       "3               date_testing                      Date of Ratings Testing   \n",
       "4            date_nc_testing               Date of Neurocognitive Testing   \n",
       "\n",
       "            scale  \n",
       "0  subject_status  \n",
       "1  subject_status  \n",
       "2  subject_status  \n",
       "3  subject_status  \n",
       "4  subject_status  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(neuropsych)\n",
    "\n",
    "def get_tables():\n",
    "# read in redcap\n",
    "    RC = pd.read_csv(os.environ.get(\"REDCAPTABLE\"),low_memory=False)\n",
    "    RCinstr = neuropsych.redcap_instruments(RC)\n",
    "\n",
    "    # read in labels of RC\n",
    "    text_file = open(os.path.join(os.environ.get(\"CODEDIR\"),\"misc/neuropsych_labels.txt\"),'r')\n",
    "    lines = text_file.read().split(\"\\n\")\n",
    "\n",
    "    labeltable = pd.DataFrame([RC.columns,lines])\n",
    "    labeltable = labeltable.transpose()\n",
    "    labeltable.columns = ['code','label']\n",
    "    labeltable['scale'] = [k for x in labeltable.code for k,v in RCinstr.iteritems() if x in v]\n",
    "    return RC, labeltable\n",
    "\n",
    "RC, labeltable = get_tables()\n",
    "\n",
    "labeltable[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I'm subsetting all psych variables, to include only certain scales, an only numerical values.  Then, I'm standardising the table, so that the weights of any data reduction are comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get a subset of all neuropsych measures \n",
    "\n",
    "def subset_tables(include=None,RC=None, labeltable=None):\n",
    "    if RC==None or labeltable==None:\n",
    "        RC, labeltable = get_tables()     \n",
    "\n",
    "    if include == None:\n",
    "        include = [\n",
    "            'legal_issues',\n",
    "            'wasi',\n",
    "            'bprse',\n",
    "            'hamd',\n",
    "            'ymrs',\n",
    "            'ldps',\n",
    "            'family_history_assessment',\n",
    "            'ctq',\n",
    "            'thq',\n",
    "            'scid',\n",
    "            'scid_face'\n",
    "            ]\n",
    "\n",
    "    # function get_measures_subset gets all non-text questions of the questionnaire\n",
    "    # (without 'has this instr been collected' and 'iscomplete')\n",
    "    neuropsych_cols = neuropsych.get_measures_subset(include=include)\n",
    "    lbl_id = np.sort([np.where(x==labeltable.code)[0][0] for x in neuropsych_cols])\n",
    "\n",
    "    #subset labeltable\n",
    "    labeltable = labeltable.iloc[lbl_id]\n",
    "    print(\"There are %i variables considered.\"%len(labeltable))\n",
    "\n",
    "    #subset subjectstable and standardise\n",
    "    FA_table = RESTABLE[labeltable.code]\n",
    "    FA_table = (FA_table - FA_table.mean()) / (FA_table.max() - FA_table.min())\n",
    "    FA_table = FA_table.fillna(0)\n",
    "    \n",
    "    return labeltable, FA_table\n",
    "\n",
    "labeltable, FA_table = subset_tables()\n",
    "FA_table[:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(labeltable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "Some columns pose problems for factor analysis (matrix singularity):\n",
    "1. There seem to be columns that are empty for all subjects.  These should be removed.\n",
    "2. There is an extremely high collinearity for certain columns (often _severity_ and _duration_ of same variable).  You can see these in the correlation plot below: the little brown squares in the ldps scale.  These should be removed or be summarised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cormat = FA_table.corr()\n",
    "cormat.fillna(0)\n",
    "unique_labels = np.unique(labeltable.scale)\n",
    "labels_num = [np.where(unique_labels==x)[0].tolist()[0] for val,x in enumerate(labeltable.scale)]\n",
    "major_ticks = [np.min(np.where(np.array(labels_num)==x))-1 for x in range(len(np.unique(labeltable.scale)))]\n",
    "minor_ticks = [np.mean(np.where(np.array(labels_num)==x))-1 for x in range(len(np.unique(labeltable.scale)))]\n",
    "fig = plt.figure(figsize=(8, 6), dpi= 100, facecolor='w', edgecolor='k')\n",
    "ax = fig.add_subplot(111)\n",
    "ax1 = ax.imshow(cormat,cmap = \"PuOr_r\",vmin=-0.2,vmax=1,aspect='auto',interpolation='nearest')\n",
    "ax.set_title('correlations between neuropsych measures')\n",
    "ax.set_xticks(major_ticks,minor=False)\n",
    "ax.set_xticks(minor_ticks,minor=True)\n",
    "ax.set_xticklabels(unique_labels,minor=True,rotation=90)\n",
    "ax.set_xticklabels(unique_labels,minor=False,visible=False)\n",
    "ax.set_yticks(major_ticks,minor=False)\n",
    "ax.set_yticks(minor_ticks,minor=True)\n",
    "ax.set_yticklabels(unique_labels,minor=True)\n",
    "ax.set_yticklabels(unique_labels,minor=False,visible=False)\n",
    "plt.colorbar(ax1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_tables(labeltable, FA_table):\n",
    "    # append Nan columns\n",
    "    dropvars = []\n",
    "    for column in FA_table:\n",
    "        FAnan = np.sum(FA_table[column]==0)\n",
    "        if FAnan==len(FA_table):\n",
    "            dropvars.append(column)\n",
    "\n",
    "    # drop variables with too high correlations\n",
    "    cormat = FA_table.corr()\n",
    "    idx = np.where(cormat > 0.95)\n",
    "    collin = [[x,y] for x,y in zip(idx[0],idx[1]) if x!=y and np.sort([x,y]).tolist()==[x,y]]\n",
    "    for pair in collin:\n",
    "        x = pair[0]\n",
    "        y = pair[1]\n",
    "        print(\"Correlation between %s and %s: %f\"%(list(FA_table)[x],list(FA_table)[y],np.array(cormat)[x,y]))\n",
    "        vardrop = list(FA_table)[y]\n",
    "        dropvars.append(vardrop)\n",
    "    \n",
    "    labeltable = labeltable.reset_index()\n",
    "    delvar = [np.where(x==labeltable.code)[0][0] for x in dropvars]\n",
    "    labeltable.drop(delvar,inplace=True)\n",
    "\n",
    "    FA_table = FA_table[labeltable.code]\n",
    "    FA_table = (FA_table - FA_table.mean()) / (FA_table.max() - FA_table.min())\n",
    "    FA_table = FA_table.fillna(0)\n",
    "\n",
    "    return labeltable, FA_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "include = [\n",
    "#    'legal_issues',\n",
    "#     'wasi',\n",
    "#     'bprse',\n",
    "#     'hamd',\n",
    "#     'ymrs',\n",
    "#     'ldps',\n",
    "     'family_history_assessment',\n",
    "#     'ctq',\n",
    "#     'thq',\n",
    "#     'scid',\n",
    "#     'scid_face'\n",
    "    ]\n",
    "labeltable, legal_table = subset_tables(include=include)\n",
    "labeltable, legal_table = clean_tables(labeltable, legal_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## look at data reduction of legal_issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R -i legal_table -o factortable\n",
    "\n",
    "maxfac <- min(dim(legal_table)[2],50)\n",
    "\n",
    "library(psych)\n",
    "factortable <- nfactors(legal_table,n=maxfac,rotation='varimax')$vss.stats\n",
    "factortable['eigen'] <- eigen(cor(legal_table))$values[1:maxfac]\n",
    "\n",
    "parallel <- fa.parallel(legal_table,plot=FALSE)\n",
    "factortable['parallel_data'] <- parallel$fa.values[1:maxfac]\n",
    "factortable['parallel_sim'] <- parallel$fa.sim[1:maxfac]\n",
    "\n",
    "vss <- vss(legal_table,n=maxfac,rotate='varimax',plot=FALSE)\n",
    "factortable['map'] <- vss$map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_factors(factortable):\n",
    "    # chi square of residual matrix\n",
    "    pvals = [stats.chi2.pdf(x,y) for x,y in zip(factortable.chisq,factortable.dof)]\n",
    "    num_c = sum(np.array(pvals)<0.05)\n",
    "    print(\"Based on the likelihood ration test, Model(N)-Model(0), the minimum number of factors is %i\"%num_c)\n",
    "\n",
    "    # likelihood ration\n",
    "    difs = np.diff(factortable.chisq)\n",
    "    difdofs = np.diff(factortable.dof)\n",
    "    pvals = [stats.chi2.pdf(-x,-y) for x,y in zip(difs,difdofs)]\n",
    "    num_c = sum(np.array(pvals)<0.05)\n",
    "    print(\"Based on the likelihood ration test, Model(b)-Model(n-1), the minimum number of factors is %i\"%(num_c+1))\n",
    "\n",
    "    # RMSEA\n",
    "    num_r = sum(factortable.RMSEA>0.05)\n",
    "    print(\"Based on the RMSEA, the minimum number of factors is %i\"%(num_r))\n",
    "\n",
    "\n",
    "    #kaiser criterion: number of eigenvalues > 1\n",
    "    num_k = sum(factortable['eigen']>1)\n",
    "    print(\"Based on the kaiser criterion, the minimum number of factors is %i\"%(num_r))\n",
    "\n",
    "    # BIC\n",
    "    num_k = np.where(np.min(factortable.BIC) == factortable.BIC)[0][0]+1\n",
    "    print(\"Based on the BIC, the minimum number of factors is %i\"%(num_k))\n",
    "    num_k = np.where(np.min(factortable.eBIC) == factortable.eBIC)[0][0]+1\n",
    "    print(\"Based on the empirical BIC, the minimum number of factors is %i\"%(num_k))\n",
    "\n",
    "    # complexity\n",
    "    num_c = np.where(np.max(factortable.complex)==factortable.complex)[0][0]+1\n",
    "    print(\"Based on the complexity, the minimum number of factors is %i\"%(num_c))\n",
    "\n",
    "    # SRMR (standardised root mean square residual)\n",
    "    num_c = sum(factortable.SRMR<0.08)\n",
    "    print(\"Based on the standardised root mean square residual, the minimum number of factors is %i\"%(num_c))\n",
    "\n",
    "    # parallel analysis: extract factors until eigen values of data < eigen vanlues of random data\n",
    "    num_c = np.min(np.where(factortable.parallel_data<factortable.parallel_sim)[0])+1\n",
    "    print(\"Based on parallel analysis, the minimum number of factors is %i\"%(num_c))\n",
    "\n",
    "    # map: minimum average partial correlation\n",
    "    num_c = np.where(np.min(factortable.map)==factortable.map)[0][0]\n",
    "    print(\"Based on the minimum average partial correlation method, the minimum number of factors is %i\"%(num_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table = legal_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluate_factors(factortable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_components = np.arange(0,14,1)\n",
    "fa = FactorAnalysis()\n",
    "fa.svd_method='randomized'\n",
    "fa.iterated_power=3\n",
    "fa.random_state = 100\n",
    "\n",
    "fa_scores = []\n",
    "maxlog = []\n",
    "for n in lp(n_components):\n",
    "    fa.n_components = n\n",
    "    fa_scores.append(np.mean(cross_val_score(fa, legal_table)))\n",
    "    fa.fit(X)\n",
    "    maxlog.append(fa.loglike_[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fa_z = (np.array(fa_scores)-np.mean(fa_scores))/np.std(fa_scores)\n",
    "maxlog_z = (np.array(maxlog)-np.mean(maxlog))/np.std(maxlog)\n",
    "n_components_fa = n_components[np.argmax(fa_scores)]\n",
    "n_components_ML = n_components[np.argmax(maxlog_z)]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(n_components, maxlog_z, cols[0], label='likelihood')\n",
    "plt.axvline(n_components_ML, color=cols[0],\n",
    "            label='FactorAnalysis loglikelihood: %d' % n_components_fa,\n",
    "            linestyle='--')\n",
    "plt.plot(n_components, fa_z, cols[1], label='FA CV scores')\n",
    "plt.axvline(n_components_fa, color=cols[1],\n",
    "            label='FactorAnalysis CV: %d' % n_components_fa,\n",
    "            linestyle='--')\n",
    "\n",
    "plt.xlabel('nb of components')\n",
    "plt.ylabel('Standardized scores')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title(\"Number of components based on cross-validation and maximum likelihood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All metrics point in different directions, but I will follow the BIC and the cross-validation that indicate that 2 factors should do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R -i legal_table -o fa_loadings\n",
    "\n",
    "library(psych)\n",
    "\n",
    "factortable <- factanal(legal_table,factors=2,rotation='varimax')\n",
    "fa_loadings = factortable$loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The factor analysis seems to indicate that the first factor indicates the presence of serious legal problems, while the second rather points to small legal problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minor_ticks = np.arange(len(labeltable.code))\n",
    "\n",
    "labelnew = [x[:50] for x in labeltable.label]\n",
    "fig = plt.figure(figsize=(5, 6), dpi= 100, facecolor='w', edgecolor='k')\n",
    "ax = fig.add_subplot(111)\n",
    "ax1 = ax.imshow(fa_loadings,\n",
    "                aspect='auto',interpolation='nearest',cmap='coolwarm')\n",
    "ax.set_yticks(np.arange(fa_loadings.shape[0]),minor=True)\n",
    "ax.set_yticklabels(labelnew,minor=True)\n",
    "plt.colorbar(ax1)\n",
    "\n",
    "\n",
    "coldict = {k:cols[idx] for idx,k in enumerate(np.unique(labeltable.scale))}\n",
    "labeltable['colors'] = [coldict[x] for x in labeltable.scale]\n",
    "for ytick, color in zip(ax.get_yticklabels(minor=True), labeltable.colors):\n",
    "    ytick.set_color(color)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fa_loadings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labeltable = labeltable.reset_index()\n",
    "delvar = [np.where(x==labeltable.code)[0][0] for x in dropvars]\n",
    "labeltable.drop(delvar,inplace=True)\n",
    "\n",
    "FA_table = FA_table[labeltable.code]\n",
    "FA_table = (FA_table - FA_table.mean()) / (FA_table.max() - FA_table.min())\n",
    "FA_table = FA_table.fillna(0)\n",
    "\n",
    "print(\"These variables are dropped: \\n%s\"%\"\\n\".join(dropvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"There are %i variables left in the neuropsych measurements\"%len(labeltable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cormat = FA_table.corr()\n",
    "cormat.fillna(0)\n",
    "unique_labels = np.unique(labeltable.scale)\n",
    "labels_num = [np.where(unique_labels==x)[0].tolist()[0] for val,x in enumerate(labeltable.scale)]\n",
    "major_ticks = [np.min(np.where(np.array(labels_num)==x))-1 for x in range(len(np.unique(labeltable.scale)))]\n",
    "minor_ticks = [np.mean(np.where(np.array(labels_num)==x))-1 for x in range(len(np.unique(labeltable.scale)))]\n",
    "fig = plt.figure(figsize=(6, 5), dpi= 100, facecolor='w', edgecolor='k')\n",
    "ax = fig.add_subplot(111)\n",
    "ax1 = ax.imshow(cormat,cmap = \"PuOr_r\",vmin=-0.2,vmax=1,aspect='auto',interpolation='nearest')\n",
    "ax.set_title('correlations between neuropsych measures')\n",
    "ax.set_xticks(major_ticks,minor=False)\n",
    "ax.set_xticks(minor_ticks,minor=True)\n",
    "ax.set_xticklabels(unique_labels,minor=True,rotation=90)\n",
    "ax.set_xticklabels(unique_labels,minor=False,visible=False)\n",
    "ax.set_yticks(major_ticks,minor=False)\n",
    "ax.set_yticks(minor_ticks,minor=True)\n",
    "ax.set_yticklabels(unique_labels,minor=True)\n",
    "ax.set_yticklabels(unique_labels,minor=False,visible=False)\n",
    "plt.colorbar(ax1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FA_table.to_csv(\"FA_table.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline prediction accuracy based on neuropsych variables\n",
    "\n",
    "Right now, the SVM is having extremely high classification accuracy.  Not really that much of a surprise, given that some forms were only filled out by psych patients (verify !) and the presence of hallucinations is pretty obvious :) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = psy_labels\n",
    "X = np.array(FA_table)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "cv = ShuffleSplit(n_splits=50, test_size=0.2, random_state=100)\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "def plot_learning_curve(estimator,title,X,y,ylim,cv=None,n_jobs=3,train_sizes = np.linspace(0.1,1.0,10)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.ylim(ylim)\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=4, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "    plt.xlabel(\"Training sample size\")\n",
    "    plt.ylabel(\"classification accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "estimator = GaussianNB()\n",
    "plot_learning_curve(estimator,\"Naive Bayes\",X,y,ylim=[0.75,1.05],cv=cv)\n",
    "\n",
    "estimator = LinearSVC()\n",
    "plot_learning_curve(estimator,\"Support Vector Classification\",X,y,ylim=[0.75,1.05],cv=cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Reduce the number of features in neuropsych measures: factor analysis with R\n",
    "\n",
    "Below, we're looping over a number of components and compute for each n the crossvalidation loglikelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%R\n",
    "# leave this in: only way to install package with R --> will need :)\n",
    "# install.packages(\"GPArotation\",repos='http://cran.us.r-project.org')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FA_table.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we're running a factor analysis in R.  The command in R gives us way more metrics than what we get with scikit-learn.  The output is `factortable`: a table with different metrics against a different number of factors.\n",
    "\n",
    "**Note** This code took too long, and was deployed to a supercomputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%R -i FA_table -o factortable\n",
    "#\n",
    "#maxfac <- 50\n",
    "#\n",
    "#library(psych)\n",
    "#factortable <- nfactors(FA_table,n=maxfac,rotation='varimax')$vss.stats\n",
    "#factortable['eigen'] <- eigen(cor(FA_table))$values[1:maxfac]\n",
    "#\n",
    "#parallel <- fa.parallel(FA_table,plot=FALSE)\n",
    "#factortable['parallel_data'] <- parallel$fa.values[1:maxfac]\n",
    "#factortable['parallel_sim'] <- parallel$fa.sim[1:maxfac]\n",
    "#\n",
    "#vss <- vss(FA_table,n=maxfac,rotate='varimax',plot=FALSE)\n",
    "#factortable['map'] <- vss$map\n",
    "\n",
    "factortable = pd.read_csv(\"04_connectome/FA_table_outR.csv\",sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "factortable[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# chi square of residual matrix\n",
    "pvals = [stats.chi2.pdf(x,y) for x,y in zip(factortable.chisq,factortable.dof)]\n",
    "num_c = sum(np.array(pvals)<0.05)\n",
    "print(\"Based on the likelihood ration test, Model(N)-Model(0), the minimum number of factors is %i\"%num_c)\n",
    "\n",
    "# likelihood ration\n",
    "difs = np.diff(factortable.chisq)\n",
    "difdofs = np.diff(factortable.dof)\n",
    "pvals = [stats.chi2.pdf(-x,-y) for x,y in zip(difs,difdofs)]\n",
    "num_c = sum(np.array(pvals)<0.05)\n",
    "print(\"Based on the likelihood ration test, Model(b)-Model(n-1), the minimum number of factors is %i\"%(num_c+1))\n",
    "\n",
    "# RMSEA\n",
    "num_r = sum(factortable.RMSEA>0.05)\n",
    "print(\"Based on the RMSEA, the minimum number of factors is %i\"%(num_r))\n",
    "\n",
    "\n",
    "#kaiser criterion: number of eigenvalues > 1\n",
    "num_k = sum(factortable['eigen']>1)\n",
    "print(\"Based on the kaiser criterion, the minimum number of factors is %i\"%(num_r))\n",
    "\n",
    "# BIC\n",
    "num_k = np.where(np.min(factortable.BIC) == factortable.BIC)[0][0]+1\n",
    "print(\"Based on the BIC, the minimum number of factors is %i\"%(num_k))\n",
    "num_k = np.where(np.min(factortable.eBIC) == factortable.eBIC)[0][0]+1\n",
    "print(\"Based on the empirical BIC, the minimum number of factors is %i\"%(num_k))\n",
    "\n",
    "# complexity\n",
    "num_c = np.where(np.max(factortable.complex)==factortable.complex)[0][0]+1\n",
    "print(\"Based on the complexity, the minimum number of factors is %i\"%(num_c))\n",
    "\n",
    "# SRMR (standardised root mean square residual)\n",
    "num_c = sum(factortable.SRMR<0.08)\n",
    "print(\"Based on the standardised root mean square residual, the minimum number of factors is %i\"%(num_c))\n",
    "\n",
    "# parallel analysis: extract factors until eigen values of data < eigen vanlues of random data\n",
    "#num_c = np.min(np.where(factortable.parallel_data<factortable.parallel_sim)[0])+1\n",
    "#print(\"Based on parallel analysis, the minimum number of factors is %i\"%(num_c))\n",
    "\n",
    "# map: minimum average partial correlation\n",
    "num_c = np.where(np.min(factortable.map)==factortable.map)[0][0]\n",
    "print(\"Based on the minimum average partial correlation method, the minimum number of factors is %i\"%(num_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "\n",
    "factortable_norm = (factortable - factortable.mean()) / factortable.std()\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "for idx,col in enumerate(factortable_norm):\n",
    "    plt.plot(factortable_norm[col],label=col,color=cols[idx],lw=3)\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### What does scikit-learn (based on cross-validation) say?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R -i FA_table -o factortable\n",
    "\n",
    "factors <- 5\n",
    "\n",
    "#print(FA_table[1])\n",
    "anal <- factanal(FA_table,factors=10,rotate='varimax',scores='Bartlett')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fa.iterated_power=3\n",
    "fa.n_components = 5\n",
    "fa.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def promax(A,k=2):\n",
    "    assert k>0\n",
    "    V, T = rotate_factors(A,'varimax')\n",
    "    H = np.abs(V)**k/V\n",
    "    S=procrustes(A,H) #np.linalg.inv(A.T.dot(A)).dot(A.T).dot(H);\n",
    "    d=np.sqrt(np.diag(np.linalg.inv(S.T.dot(S))));\n",
    "    D=np.diag(d)\n",
    "    T=np.linalg.inv(S.dot(D)).T\n",
    "    return A.dot(T), T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def varimax(Phi, gamma = 1, q = 20, tol = 1e-6):\n",
    "    from numpy import eye, asarray, dot, sum, diag\n",
    "    from numpy.linalg import svd\n",
    "    p,k = Phi.shape\n",
    "    R = eye(k)\n",
    "    d=0\n",
    "    for i in xrange(q):\n",
    "        d_old = d\n",
    "        Lambda = dot(Phi, R)\n",
    "        u,s,vh = svd(dot(Phi.T,asarray(Lambda)**3 - (gamma/p) * dot(Lambda, diag(diag(dot(Lambda.T,Lambda))))))\n",
    "        R = dot(u,vh)\n",
    "        d = sum(s)\n",
    "        if d/d_old < tol: break\n",
    "    return dot(Phi, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_red = varimax(fa.components_)\n",
    "#X_red = fa.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "minor_ticks = np.arange(len(labeltable.code))\n",
    "\n",
    "labelnew = [x[:50] for x in labeltable.label]\n",
    "fig = plt.figure(figsize=(10, 30), dpi= 100, facecolor='w', edgecolor='k')\n",
    "ax = fig.add_subplot(111)\n",
    "ax1 = ax.imshow(np.transpose(X_red),\n",
    "                aspect='auto',interpolation='nearest',cmap='coolwarm')\n",
    "ax.set_yticks(np.arange(X_red.shape[1]),minor=True)\n",
    "ax.set_yticklabels(labelnew,minor=True)\n",
    "plt.colorbar(ax1)\n",
    "\n",
    "\n",
    "coldict = {k:cols[idx] for idx,k in enumerate(np.unique(labeltable.scale))}\n",
    "labeltable['colors'] = [coldict[x] for x in labeltable.scale]\n",
    "for ytick, color in zip(ax.get_yticklabels(minor=True), labeltable.colors):\n",
    "    ytick.set_color(color)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.arange(len(neuropsych_cols))[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "upid = np.triu_indices(statT.shape[0])\n",
    "len(upid[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "connectomes = np.load(os.path.join(CONDIR,'derivatives/connectomes.npy'))\n",
    "passid = np.where(np.logical_and(table.MOTION_pass == 1, table.MRIQC_pass == 1))[0]\n",
    "connectomes = connectomes[:,:,passid]\n",
    "subtable = table.iloc[list(passid),:]\n",
    "subtable = subtable.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wide = np.zeros([len(passid),len(upid[0])])\n",
    "for sub in range(len(passid)):\n",
    "    wide[sub,:] = connectomes[:,:,sub][upid]\n",
    "wide = pd.DataFrame(wide)\n",
    "print(wide.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canonical correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "cca = CCA(n_components = 4)\n",
    "cca.fit(X_new,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cca.get_params()\n",
    "#cca.x_scores_.shape\n",
    "cca.x_weights_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cca.y_weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rotate_factors(A, method, *method_args, **algorithm_kwargs):\n",
    "    r\"\"\"\n",
    "    Subroutine for orthogonal and oblique rotation of the matrix :math:`A`.\n",
    "    For orthogonal rotations :math:`A` is rotated to :math:`L` according to\n",
    "    \n",
    "    .. math::\n",
    "        L =  AT,\n",
    "        \n",
    "    where :math:`T` is an orthogonal matrix. And, for oblique rotations\n",
    "    :math:`A` is rotated to :math:`L` according to\n",
    "    \n",
    "    .. math::\n",
    "        L =  A(T^*)^{-1},\n",
    "    where :math:`T` is a normal matrix.\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    What follows is a list of available methods. Depending on the method\n",
    "    additional argument are required and different algorithms\n",
    "    are available. The algorithm_kwargs are additional keyword arguments\n",
    "    passed to the selected algorithm (see the parameters section).\n",
    "    Unless stated otherwise, only the gpa and\n",
    "    gpa_der_free algorithm are available.\n",
    "    \n",
    "    Below,\n",
    "    \n",
    "    * :math:`L` is a :math:`p\\times k` matrix;\n",
    "    * :math:`N` is :math:`k\\times k` matrix with zeros on the diagonal and ones elsewhere;\n",
    "    * :math:`M` is :math:`p\\times p` matrix with zeros on the diagonal and ones elsewhere;\n",
    "    * :math:`C` is a :math:`p\\times p` matrix with elements equal to :math:`1/p`;\n",
    "    * :math:`(X,Y)=\\operatorname{Tr}(X^*Y)` is the Frobenius norm;\n",
    "    * :math:`\\circ` is the element-wise product or Hadamard product.\n",
    "    \n",
    "    oblimin : orthogonal or oblique rotation that minimizes\n",
    "        .. math::\n",
    "            \\phi(L) = \\frac{1}{4}(L\\circ L,(I-\\gamma C)(L\\circ L)N).\n",
    "            \n",
    "        For orthogonal rotations:\n",
    "        * :math:`\\gamma=0` corresponds to quartimax,\n",
    "        * :math:`\\gamma=\\frac{1}{2}` corresponds to biquartimax,\n",
    "        * :math:`\\gamma=1` corresponds to varimax,\n",
    "        * :math:`\\gamma=\\frac{1}{p}` corresponds to equamax.\n",
    "        For oblique rotations rotations:\n",
    "    \n",
    "        * :math:`\\gamma=0` corresponds to quartimin,\n",
    "        * :math:`\\gamma=\\frac{1}{2}` corresponds to biquartimin.\n",
    "        \n",
    "        method_args:\n",
    "        \n",
    "        gamma : float\n",
    "            oblimin family parameter\n",
    "        rotation_method : string\n",
    "            should be one of {orthogonal, oblique}\n",
    "    \n",
    "    orthomax : orthogonal rotation that minimizes\n",
    "        .. math::\n",
    "            \\phi(L) = -\\frac{1}{4}(L\\circ L,(I-\\gamma C)(L\\circ L)),\n",
    "            \n",
    "        where :math:`0\\leq\\gamma\\leq1`. The orthomax family is equivalent to \n",
    "        the oblimin family (when restricted to orthogonal rotations). Furthermore,\n",
    "        * :math:`\\gamma=0` corresponds to quartimax,\n",
    "        * :math:`\\gamma=\\frac{1}{2}` corresponds to biquartimax,\n",
    "        * :math:`\\gamma=1` corresponds to varimax,\n",
    "        * :math:`\\gamma=\\frac{1}{p}` corresponds to equamax.\n",
    "        \n",
    "        method_args:\n",
    "        \n",
    "        gamma : float (between 0 and 1)\n",
    "            orthomax family parameter\n",
    "    \n",
    "    CF : Crawford-Ferguson family for orthogonal and oblique rotation wich minimizes:\n",
    "        .. math::\n",
    "            \\phi(L) =\\frac{1-\\kappa}{4} (L\\circ L,(L\\circ L)N)\n",
    "                      -\\frac{1}{4}(L\\circ L,M(L\\circ L)),\n",
    "                      \n",
    "        where :math:`0\\leq\\kappa\\leq1`. For orthogonal rotations the oblimin\n",
    "        (and orthomax) family of rotations is equivalent to the Crawford-Ferguson family.\n",
    "        To be more precise:\n",
    "    \n",
    "        * :math:`\\kappa=0` corresponds to quartimax,\n",
    "        * :math:`\\kappa=\\frac{1}{p}` corresponds to varimax,\n",
    "        * :math:`\\kappa=\\frac{k-1}{p+k-2}` corresponds to parsimax,\n",
    "        * :math:`\\kappa=1` corresponds to factor parsimony.\n",
    "        \n",
    "        method_args:\n",
    "        \n",
    "        kappa : float (between 0 and 1)\n",
    "            Crawford-Ferguson family parameter\n",
    "        rotation_method : string\n",
    "            should be one of {orthogonal, oblique}\n",
    "    \n",
    "    quartimax : orthogonal rotation method\n",
    "        minimizes the orthomax objective with :math:`\\gamma=0`\n",
    "        \n",
    "    biquartimax : orthogonal rotation method\n",
    "        minimizes the orthomax objective with :math:`\\gamma=\\frac{1}{2}`\n",
    "        \n",
    "    varimax : orthogonal rotation method\n",
    "        minimizes the orthomax objective with :math:`\\gamma=1`\n",
    "        \n",
    "    equamax : orthogonal rotation method\n",
    "        minimizes the orthomax objective with :math:`\\gamma=\\frac{1}{p}`\n",
    "        \n",
    "    parsimax : orthogonal rotation method\n",
    "        minimizes the Crawford-Ferguson family objective with :math:`\\kappa=\\frac{k-1}{p+k-2}`\n",
    "        \n",
    "    parsimony : orthogonal rotation method\n",
    "        minimizes the Crawford-Ferguson family objective with :math:`\\kappa=1`\n",
    "    \n",
    "    quartimin : oblique rotation method that minimizes\n",
    "        minimizes the oblimin objective with :math:`\\gamma=0`      \n",
    "    quartimin : oblique rotation method that minimizes\n",
    "        minimizes the oblimin objective with :math:`\\gamma=\\frac{1}{2}`   \n",
    "        \n",
    "    target : orthogonal or oblique rotation that rotates towards a target matrix :math:`H` by minimizing the objective\n",
    "        .. math::\n",
    "            \\phi(L) =\\frac{1}{2}\\|L-H\\|^2.\n",
    "        \n",
    "        method_args:\n",
    "        \n",
    "        H : numpy matrix\n",
    "            target matrix\n",
    "        rotation_method : string\n",
    "            should be one of {orthogonal, oblique}\n",
    "        For orthogonal rotations the algorithm can be set to analytic in which case\n",
    "        the following keyword arguments are available:\n",
    "        \n",
    "        full_rank : boolean (default False)\n",
    "            if set to true full rank is assumed    \n",
    "    partial_target : orthogonal (default) or oblique rotation that partially rotates\n",
    "        towards a target matrix :math:`H` by minimizing the objective:\n",
    "        \n",
    "        .. math::\n",
    "            \\phi(L) =\\frac{1}{2}\\|W\\circ(L-H)\\|^2.\n",
    "        \n",
    "        method_args:\n",
    "        \n",
    "        H : numpy matrix\n",
    "            target matrix\n",
    "        W : numpy matrix (default matrix with equal weight one for all entries)\n",
    "            matrix with weights, entries can either be one or zero\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    A : numpy matrix (default None)\n",
    "        non rotated factors\n",
    "    method : string\n",
    "        should be one of the methods listed above\n",
    "    method_args : list\n",
    "        additional arguments that should be provided with each method\n",
    "    algorithm_kwargs : dictionary\n",
    "        algorithm : string (default gpa)\n",
    "            should be one of:\n",
    "            \n",
    "            * 'gpa': a numerical method\n",
    "            * 'gpa_der_free': a derivative free numerical method\n",
    "            * 'analytic' : an analytic method\n",
    "        Depending on the algorithm, there are algorithm specific keyword\n",
    "        arguments. For the gpa and gpa_der_free, the following\n",
    "        keyword arguments are available:\n",
    "        \n",
    "        max_tries : integer (default 501)\n",
    "            maximum number of iterations\n",
    "        tol : float\n",
    "            stop criterion, algorithm stops if Frobenius norm of gradient is\n",
    "            smaller then tol\n",
    "        For analytic, the supporeted arguments depend on the method, see above.\n",
    "            \n",
    "        See the lower level functions for more details.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    The tuple :math:`(L,T)`\n",
    "    \n",
    "    Examples\n",
    "    -------\n",
    "    >>> A = np.random.randn(8,2)\n",
    "    >>> L, T = rotate_factors(A,'varimax')\n",
    "    >>> np.allclose(L,A.dot(T))\n",
    "    >>> L, T = rotate_factors(A,'orthomax',0.5)\n",
    "    >>> np.allclose(L,A.dot(T))\n",
    "    >>> L, T = rotate_factors(A,'quartimin',0.5)\n",
    "    >>> np.allclose(L,A.dot(np.linalg.inv(T.T)))\n",
    "    \"\"\"\n",
    "    if 'algorithm' in algorithm_kwargs:\n",
    "        algorithm = algorithm_kwargs['algorithm']\n",
    "        algorithm_kwargs.pop('algorithm')\n",
    "    else:\n",
    "        algorithm = 'gpa'\n",
    "    assert 'rotation_method' not in algorithm_kwargs, 'rotation_method cannot be provided as keyword argument'\n",
    "    L=None\n",
    "    T=None\n",
    "    ff=None\n",
    "    vgQ=None\n",
    "    p,k = A.shape\n",
    "    #set ff or vgQ to appropriate objective function, compute solution using recursion or analytically compute solution\n",
    "    if method == 'orthomax':\n",
    "        assert len(method_args)==1, 'Only %s family parameter should be provided' % method\n",
    "        rotation_method='orthogonal'\n",
    "        gamma = method_args[0]\n",
    "        if algorithm =='gpa':\n",
    "            vgQ=lambda L=None, A=None, T=None: gr.orthomax_objective(L=L,A=A,T=T,\n",
    "                                                                     gamma=gamma,\n",
    "                                                                     return_gradient=True)\n",
    "        elif algorithm =='gpa_der_free':\n",
    "            ff = lambda L=None, A=None, T=None: gr.orthomax_objective(L=L,A=A,T=T,\n",
    "                                                                      gamma=gamma,\n",
    "                                                                      return_gradient=False)\n",
    "        else:\n",
    "            raise ValueError('Algorithm %s is not possible for %s rotation' % (algorithm, method))\n",
    "    elif method == 'oblimin':\n",
    "        assert len(method_args)==2, 'Both %s family parameter and rotation_method should be provided' % method\n",
    "        rotation_method=method_args[1]\n",
    "        assert rotation_method in ['orthogonal','oblique'], 'rotation_method should be one of {orthogonal, oblique}'\n",
    "        gamma = method_args[0]\n",
    "        if algorithm =='gpa':\n",
    "            vgQ=lambda L=None, A=None, T=None: gr.oblimin_objective(L=L,A=A,T=T,\n",
    "                                                                    gamma=gamma,\n",
    "                                                                    return_gradient=True)\n",
    "        elif algorithm =='gpa_der_free':\n",
    "            ff = lambda L=None, A=None, T=None: gr.oblimin_objective(L=L,A=A,T=T,\n",
    "                                                                     gamma=gamma,\n",
    "                                                                     rotation_method=rotation_method,\n",
    "                                                                     return_gradient=False)\n",
    "        else:\n",
    "            raise ValueError('Algorithm %s is not possible for %s rotation' % (algorithm, method))\n",
    "    elif method == 'CF':\n",
    "        assert len(method_args)==2, 'Both %s family parameter and rotation_method should be provided' % method\n",
    "        rotation_method=method_args[1]\n",
    "        assert rotation_method in ['orthogonal','oblique'], 'rotation_method should be one of {orthogonal, oblique}'\n",
    "        kappa = method_args[0]\n",
    "        if algorithm =='gpa':\n",
    "            vgQ=lambda L=None, A=None, T=None: gr.CF_objective(L=L,A=A,T=T,\n",
    "                                                               kappa=kappa,\n",
    "                                                               rotation_method=rotation_method,\n",
    "                                                               return_gradient=True)\n",
    "        elif algorithm =='gpa_der_free':\n",
    "            ff = lambda L=None, A=None, T=None: gr.CF_objective(L=L,A=A,T=T,\n",
    "                                                                kappa=kappa,\n",
    "                                                                rotation_method=rotation_method,\n",
    "                                                                return_gradient=False)\n",
    "        else:\n",
    "            raise ValueError('Algorithm %s is not possible for %s rotation' % (algorithm, method))\n",
    "    elif method == 'quartimax':\n",
    "        return rotate_factors(A, 'orthomax', 0, **algorithm_kwargs)\n",
    "    elif method == 'biquartimax':\n",
    "        return rotate_factors(A, 'orthomax', 0.5, **algorithm_kwargs)\n",
    "    elif method == 'varimax':\n",
    "        return rotate_factors(A, 'orthomax', 1, **algorithm_kwargs)\n",
    "    elif method == 'equamax':\n",
    "        return rotate_factors(A, 'orthomax', 1/p, **algorithm_kwargs)\n",
    "    elif method == 'parsimax':\n",
    "        return rotate_factors(A, 'CF', (k-1)/(p+k-2), 'orthogonal', **algorithm_kwargs)    \n",
    "    elif method == 'parsimony':\n",
    "        return rotate_factors(A, 'CF', 1, 'orthogonal', **algorithm_kwargs)    \n",
    "    elif method == 'quartimin':\n",
    "        return rotate_factors(A, 'oblimin', 0, 'oblique', **algorithm_kwargs)\n",
    "    elif method == 'biquartimin':\n",
    "        return rotate_factors(A, 'oblimin', 0.5, 'oblique', **algorithm_kwargs)\n",
    "    elif method == 'target':\n",
    "        assert len(method_args)==2, 'only the rotation target and orthogonal/oblique should be provide for %s rotation' % method\n",
    "        H=method_args[0]\n",
    "        rotation_method=method_args[1]\n",
    "        assert rotation_method in ['orthogonal','oblique'], 'rotation_method should be one of {orthogonal, oblique}'\n",
    "        if algorithm =='gpa':\n",
    "            vgQ=lambda L=None, A=None, T=None: gr.vgQ_target(H,L=L,A=A,T=T,rotation_method=rotation_method)\n",
    "        elif algorithm =='gpa_der_free':\n",
    "            ff = lambda L=None, A=None, T=None: gr.ff_target(H,L=L,A=A,T=T,rotation_method=rotation_method)\n",
    "        elif algorithm =='analytic':\n",
    "            assert rotation_method == 'orthogonal', 'For analytic %s rotation only orthogonal rotation is supported'\n",
    "            T= ar.target_rotation(A,H,**algorithm_kwargs)\n",
    "        else:\n",
    "            raise ValueError('Algorithm %s is not possible for %s rotation' % (algorithm, method))\n",
    "    elif method == 'partial_target':\n",
    "        assert len(method_args)==2, '2 additional arguments are expected for %s rotation' % method\n",
    "        H=method_args[0]\n",
    "        W=method_args[1]\n",
    "        rotation_method='orthogonal'\n",
    "        if algorithm =='gpa':\n",
    "            vgQ=lambda L=None, A=None, T=None: gr.vgQ_partial_target(H,W=W,L=L,A=A,T=T)\n",
    "        elif algorithm =='gpa_der_free':\n",
    "            ff = lambda L=None, A=None, T=None: gr.ff_partial_target(H,W=W,L=L,A=A,T=T)\n",
    "        else:\n",
    "            raise ValueError('Algorithm %s is not possible for %s rotation' % (algorithm, method))\n",
    "    else:\n",
    "        raise ValueError('Invalid method')\n",
    "    #compute L and T if not already done\n",
    "    if T is None:\n",
    "        L, phi, T, table = gr.GPA(A, vgQ=vgQ, ff=ff, rotation_method=rotation_method, **algorithm_kwargs)\n",
    "    if L is None:\n",
    "        assert T is not None, 'Cannot compute L without T'\n",
    "        L=gr.rotateA(A,T,rotation_method=rotation_method)\n",
    "    #return\n",
    "    return L, T\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
